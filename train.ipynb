{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import read_quote      as rq\n",
    "import stock_functions as sf\n",
    "import df_visualizations as dv\n",
    "import remap_values as rv\n",
    "\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import BaggingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "roll_nums = [5,15]#[3,5,7,10,12,15,18,20,22,25,28,30] # Generate an algorithm for predicting every few days\n",
    "mom_nums  = [3,5,10,20,30]                     # Momentum has many good tracers\n",
    "rsi_nums  = [10,15]                            # Good for some long term trends\n",
    "band_nums = [5,10,15,20,25]                    # A few trace different areas well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#inpFileList = ['aapl','ba','farm','hes','ibm','mas','sbux','tgt']\n",
    "inpFileList = ['tgt','aapl','ba','farm','hes','ibm','mas','sbux']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt\n",
      "\tReading quote:  quotes/tgt.csv\n",
      "\tDone.\n",
      "\n",
      "\tGenerating variables...\n",
      "\tDone.\n",
      "\n",
      "\tNormalizing variables...\n",
      "\tDone.\n",
      "\n",
      "\tGenerating target variables...\n",
      "\tDone.\n",
      "\n",
      "\tTraining data...\n",
      "\tDone.\n",
      "Neural Network internal accuracy of :  0.1460\n",
      "Random Forest  internal accuracy of :  0.8456\n",
      "Bagging        internal accuracy of :  0.9033\n",
      "\n",
      "aapl\n",
      "\tReading quote:  quotes/aapl.csv\n",
      "\tDone.\n",
      "\n",
      "\tGenerating variables...\n",
      "\tDone.\n",
      "\n",
      "\tNormalizing variables...\n",
      "\tDone.\n",
      "\n",
      "\tGenerating target variables...\n",
      "\tDone.\n",
      "\n",
      "\tTraining data...\n",
      "\tDone.\n",
      "Neural Network internal accuracy of :  0.2119\n",
      "Random Forest  internal accuracy of :  0.6690\n",
      "Bagging        internal accuracy of :  0.7175\n",
      "\n",
      "ba\n",
      "\tReading quote:  quotes/ba.csv\n",
      "\tDone.\n",
      "\n",
      "\tGenerating variables...\n",
      "\tDone.\n",
      "\n",
      "\tNormalizing variables...\n",
      "\tDone.\n",
      "\n",
      "\tGenerating target variables...\n",
      "\tDone.\n",
      "\n",
      "\tTraining data...\n",
      "\tDone.\n",
      "Neural Network internal accuracy of : -0.6207\n",
      "Random Forest  internal accuracy of :  0.5805\n",
      "Bagging        internal accuracy of :  0.6157\n",
      "\n",
      "farm\n",
      "\tReading quote:  quotes/farm.csv\n",
      "\tDone.\n",
      "\n",
      "\tGenerating variables...\n",
      "\tDone.\n",
      "\n",
      "\tNormalizing variables...\n",
      "\tDone.\n",
      "\n",
      "\tGenerating target variables...\n",
      "\tDone.\n",
      "\n",
      "\tTraining data...\n",
      "\tDone.\n",
      "Neural Network internal accuracy of :  0.1725\n",
      "Random Forest  internal accuracy of :  0.5351\n",
      "Bagging        internal accuracy of :  0.5663\n",
      "\n",
      "hes\n",
      "\tReading quote:  quotes/hes.csv\n",
      "\tDone.\n",
      "\n",
      "\tGenerating variables...\n",
      "\tDone.\n",
      "\n",
      "\tNormalizing variables...\n",
      "\tDone.\n",
      "\n",
      "\tGenerating target variables...\n",
      "\tDone.\n",
      "\n",
      "\tTraining data...\n",
      "\tDone.\n",
      "Neural Network internal accuracy of : -0.0407\n",
      "Random Forest  internal accuracy of :  0.5166\n",
      "Bagging        internal accuracy of :  0.5428\n",
      "\n",
      "ibm\n",
      "\tReading quote:  quotes/ibm.csv\n",
      "\tDone.\n",
      "\n",
      "\tGenerating variables...\n",
      "\tDone.\n",
      "\n",
      "\tNormalizing variables...\n",
      "\tDone.\n",
      "\n",
      "\tGenerating target variables...\n",
      "\tDone.\n",
      "\n",
      "\tTraining data...\n",
      "\tDone.\n",
      "Neural Network internal accuracy of : -0.5236\n",
      "Random Forest  internal accuracy of :  0.3551\n",
      "Bagging        internal accuracy of :  0.3757\n",
      "\n",
      "mas\n",
      "\tReading quote:  quotes/mas.csv\n",
      "\tDone.\n",
      "\n",
      "\tGenerating variables...\n",
      "\tDone.\n",
      "\n",
      "\tNormalizing variables...\n",
      "\tDone.\n",
      "\n",
      "\tGenerating target variables...\n",
      "\tDone.\n",
      "\n",
      "\tTraining data...\n",
      "\tDone.\n",
      "Neural Network internal accuracy of :  0.3415\n",
      "Random Forest  internal accuracy of :  0.4809\n",
      "Bagging        internal accuracy of :  0.5008\n",
      "\n",
      "sbux\n",
      "\tReading quote:  quotes/sbux.csv\n",
      "\tDone.\n",
      "\n",
      "\tGenerating variables...\n",
      "\tDone.\n",
      "\n",
      "\tNormalizing variables...\n",
      "\tDone.\n",
      "\n",
      "\tGenerating target variables...\n",
      "\tDone.\n",
      "\n",
      "Neural Network fit accuracy: -0.6431\n",
      "Random Forest  fit accuracy:  0.3205\n",
      "Bagging        fit accuracy:  0.3098\n"
     ]
    }
   ],
   "source": [
    "rf_estimators_base = 10 # Increase number of estimators each time\n",
    "rf_estimators      = rf_estimators_base\n",
    "\n",
    "nn_reg = MLPRegressor         ( hidden_layer_sizes=200, warm_start=True )\n",
    "rf_reg = RandomForestRegressor( n_estimators=rf_estimators, min_samples_split=10, warm_start=True )\n",
    "ba_reg = BaggingRegressor     ( n_estimators=rf_estimators, warm_start=True)\n",
    "#en_reg = ElasticNet           ( warm_start= True )\n",
    "\n",
    "# Loop over inpFileList, so multiple stocks being trained\n",
    "for inpFile in inpFileList:\n",
    "\n",
    "    fileName = 'quotes/' + inpFile + '.csv'\n",
    "\n",
    "\n",
    "    print inpFile\n",
    "\n",
    "    print '\\tReading quote: ', fileName\n",
    "\n",
    "    my_quote = rq.readQuote( fileName )\n",
    "\n",
    "    print '\\tDone.\\n'\n",
    "\n",
    "\n",
    "    ##################################################\n",
    "    ############Generate Variables####################\n",
    "    ##################################################\n",
    "\n",
    "    print '\\tGenerating variables...'\n",
    "\n",
    "    # The variables we will use for the training data\n",
    "    diffs = sf.generate_differentials   ( my_quote            ).drop('diff_v',axis=1)\n",
    "    moms  = sf.generate_momentum_close  ( my_quote, mom_nums  )\n",
    "    rsis  = sf.generate_rsi             ( my_quote, rsi_nums  )\n",
    "    bands = sf.generate_bollinger_bands ( my_quote, band_nums )\n",
    "\n",
    "\n",
    "    print '\\tDone.\\n'\n",
    "\n",
    "    ##################################################\n",
    "    #############Normalize Variables##################\n",
    "    ##################################################\n",
    "\n",
    "    print '\\tNormalizing variables...'\n",
    "\n",
    "    # Differentials in a day can be smart scaled\n",
    "    diffs['diff_hl'] = np.log10( diffs['diff_hl'] )\n",
    "    for col in diffs.columns:\n",
    "        diffs[col] = rv.smart_scale( diffs, col, show_plot=False )\n",
    "\n",
    "    # Momentums can also be smart scaled\n",
    "    for col in moms.columns:\n",
    "        moms[col] = rv.smart_scale( moms, col, show_plot=False )\n",
    "\n",
    "    # RSIs have natural distribution centered at 0.5, scale accordingly\n",
    "    for col in rsis.columns:\n",
    "        rsis[col] = ( rsis[col] - 0.5 ) / rsis[col].std()\n",
    "\n",
    "    # Bands also centered at 0.5, use stdev of bollinger band of 0.25 for scaling\n",
    "    for col in bands.columns:\n",
    "        bands[col] = ( bands[col] - 0.5 ) / 0.5\n",
    "\n",
    "\n",
    "    print '\\tDone.\\n'\n",
    "\n",
    "    var_df_list = [ diffs, moms, rsis, bands ]\n",
    "    all_train_variables = reduce( lambda left,right: left.join(right,how='inner'), var_df_list )\n",
    "\n",
    "\n",
    "    ##################################################\n",
    "    #############Generate Target Variables############\n",
    "    ##################################################\n",
    "\n",
    "\n",
    "    print '\\tGenerating target variables...'\n",
    "\n",
    "    # The target variables are stored in the data frame\n",
    "    rolls = sf.generate_rolling_close   ( my_quote, roll_nums, onlyMean=True )\n",
    "    for i in roll_nums:\n",
    "        rolls['close_mean_'+str(i)] = ( rolls['close_mean_'+str(i)].shift(i) / rolls['close_mean_'+str(i)] - 1 )\n",
    "    rolls = rolls.replace( [np.inf, -np.inf], np.nan )\n",
    "\n",
    "\n",
    "    # The predicted value column heads\n",
    "    target_list = rolls.columns.values\n",
    "\n",
    "\n",
    "    print '\\tDone.\\n'\n",
    "\n",
    "\n",
    "    ##################################################\n",
    "    ##########Train the model on all but last#########\n",
    "    ##################################################\n",
    "\n",
    "\n",
    "    # Loop over target list and train multiple machine learning methods on it:\n",
    "    #for target_col in target_list:\n",
    "\n",
    "    # Generate a target values and variable values\n",
    "    target_col = target_list[-1]\n",
    "\n",
    "    # Drops rows containing na, and reverse order for training/testing\n",
    "    big_df = rolls[target_col].dropna().to_frame().join( all_train_variables, how='inner' )\n",
    "    big_df = big_df[::-1]\n",
    "\n",
    "    # Break up target and variables, not really a train_x/test_x since training over whole datasets\n",
    "    target_values   = big_df[target_col].values\n",
    "    variable_values = big_df.drop( target_col, axis=1 ).values\n",
    "\n",
    "    # Only fit using all but last data\n",
    "    if ( inpFile != inpFileList[-1] ):\n",
    "    \n",
    "        print '\\tTraining data...'\n",
    "\n",
    "        rf_reg.set_params( n_estimators=rf_estimators )\n",
    "        ba_reg.set_params( n_estimators=rf_estimators )\n",
    "        \n",
    "        rf_estimators = rf_estimators + rf_estimators_base\n",
    "        \n",
    "        nn_reg.fit( variable_values, target_values )\n",
    "        rf_reg.fit( variable_values, target_values )\n",
    "        ba_reg.fit( variable_values, target_values )\n",
    "#        en_reg.fit( variable_values, target_values )\n",
    "\n",
    "        print '\\tDone.'\n",
    "\n",
    "        print 'Neural Network internal accuracy of : %7.4f' % ( nn_reg.score( variable_values, target_values ) )\n",
    "        print 'Random Forest  internal accuracy of : %7.4f' % ( rf_reg.score( variable_values, target_values ) )\n",
    "        print 'Bagging        internal accuracy of : %7.4f' % ( ba_reg.score( variable_values, target_values ) )\n",
    "#        print 'Elastic Net    internal accuracy of : %7.4f' % ( en_reg.score( variable_values, target_values ) )\n",
    "        print ''\n",
    "        \n",
    "    else:\n",
    "       print 'Neural Network fit accuracy: %7.4f' % nn_reg.score( variable_values, target_values )\n",
    "       print 'Random Forest  fit accuracy: %7.4f' % rf_reg.score( variable_values, target_values )\n",
    "       print 'Bagging        fit accuracy: %7.4f' % ba_reg.score( variable_values, target_values )\n",
    "#       print 'Elastic Net    fit accuracy: %7.4f' % en_reg.score( variable_values, target_values )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_frame = pd.DataFrame( {'true':target_values} )\n",
    "plot_frame['random_forest' ] = rf_reg.predict( variable_values ) - plot_frame['true']\n",
    "plot_frame['neural_network'] = nn_reg.predict( variable_values ) - plot_frame['true']\n",
    "plot_frame['bagging'       ] = ba_reg.predict( variable_values ) - plot_frame['true']\n",
    "plot_frame['average'       ] = (plot_frame['random_forest' ]+\n",
    "                                plot_frame['bagging'       ]+\n",
    "                                plot_frame['neural_network'])/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ll   = 0.5\n",
    "reg1 = 'random_forest'\n",
    "reg2 = 'neural_network'\n",
    "reg3 = 'bagging'\n",
    "reg4 = 'average'\n",
    "\n",
    "ax1 = plot_frame.plot(kind='scatter', x='true', y=reg1, color='g', alpha=0.3, label=reg1 )    \n",
    "ax2 = plot_frame.plot(kind='scatter', x='true', y=reg2, color='r', alpha=0.3, label=reg2, ax=ax1)\n",
    "ax3 = plot_frame.plot(kind='scatter', x='true', y=reg3, color='b', alpha=0.3, label=reg3, ax=ax1)\n",
    "ax4 = plot_frame.plot(kind='scatter', x='true', y=reg4, color='k', alpha=0.3, label=reg4, ax=ax1)\n",
    "\n",
    "#ax1.plot( [-ll,ll], [-ll,ll], color='k' )\n",
    "ax1.set_ybound( [-ll,ll] )\n",
    "ax1.set_xlabel( 'True Value' )\n",
    "ax1.set_ylabel( 'predicted-true' )\n",
    "ax1.legend( loc=2 )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'remap_values' from 'remap_values.py'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(rv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1 accuracy: 0.3860  ,  -0.9372  {'min_samples_split': 10, 'n_estimators': 150}\n",
      "Fold  2 accuracy: -1.0783  ,  -0.6193  {'min_samples_split': 10, 'n_estimators': 100}\n",
      "Fold  3 accuracy: 0.0409  ,  -0.0810  {'min_samples_split': 5, 'n_estimators': 150}\n",
      "Fold  4 accuracy: -0.4852  ,  0.2282  {'min_samples_split': 10, 'n_estimators': 150}\n",
      "Fold  5 accuracy: 0.4500  ,  0.1066  {'min_samples_split': 2, 'n_estimators': 100}\n",
      "Fold  6 accuracy: -1.0947  ,  -0.0031  {'min_samples_split': 5, 'n_estimators': 100}\n",
      "Fold  7 accuracy: 0.2525  ,  0.2529  {'min_samples_split': 5, 'n_estimators': 100}\n",
      "Fold  8 accuracy: -0.3233  ,  0.2456  {'min_samples_split': 2, 'n_estimators': 150}\n",
      "Fold  9 accuracy: -0.0040  ,  0.3159  {'min_samples_split': 10, 'n_estimators': 150}\n",
      "Fold 10 accuracy: 0.1032  ,  0.3911  {'min_samples_split': 5, 'n_estimators': 150}\n",
      " \n",
      "Found  7  unique parameter combinations\n",
      " \n",
      "Clf  0 Final Accuracy: 0.1589 +/- 0.5585\n",
      "Clf  1 Final Accuracy: 0.1555 +/- 0.5519\n",
      "Clf  2 Final Accuracy: 0.1228 +/- 0.5488\n",
      "Clf  3 Final Accuracy: 0.1339 +/- 0.5731\n",
      "Clf  4 Final Accuracy: 0.1706 +/- 0.5388\n",
      "Clf  5 Final Accuracy: 0.1433 +/- 0.5470\n",
      "Clf  6 Final Accuracy: 0.1412 +/- 0.5454\n",
      " \n",
      "Using CLF with accuracy:   0.170643\n",
      "CLF params:  {'warm_start': False, 'oob_score': False, 'n_jobs': 1, 'verbose': 0, 'max_leaf_nodes': None, 'bootstrap': True, 'min_samples_leaf': 1, 'n_estimators': 100, 'min_samples_split': 5, 'min_weight_fraction_leaf': 0.0, 'criterion': 'mse', 'random_state': None, 'min_impurity_split': 1e-07, 'max_features': 'auto', 'max_depth': None}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=5, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_grid = {'n_estimators':[100,150],\n",
    "           'min_samples_split':[2,5,10]}\n",
    "rv.optimize_timeseries_reg( RandomForestRegressor(), variable_values, target_values, my_grid )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1 accuracy: 0.3612  ,  -1.0870  {'l1_ratio': 0.1, 'fit_intercept': False}\n",
      "Fold  2 accuracy: -1.7827  ,  -0.3216  {'l1_ratio': 0.1, 'fit_intercept': False}\n",
      "Fold  3 accuracy: 0.2242  ,  -0.3600  {'l1_ratio': 0.1, 'fit_intercept': True}\n",
      "Fold  4 accuracy: 0.0605  ,  0.1006  {'l1_ratio': 0.1, 'fit_intercept': True}\n",
      "Fold  5 accuracy: 0.0573  ,  0.0787  {'l1_ratio': 0.1, 'fit_intercept': True}\n",
      "Fold  6 accuracy: -2.2429  ,  -0.0640  {'l1_ratio': 0.1, 'fit_intercept': True}\n",
      "Fold  7 accuracy: -0.0003  ,  0.0499  {'l1_ratio': 0.1, 'fit_intercept': True}\n",
      "Fold  8 accuracy: -0.0546  ,  0.0565  {'l1_ratio': 0.1, 'fit_intercept': True}\n",
      "Fold  9 accuracy: -1.1303  ,  -0.0286  {'l1_ratio': 0.1, 'fit_intercept': True}\n",
      "Fold 10 accuracy: -0.0031  ,  0.0199  {'l1_ratio': 0.1, 'fit_intercept': True}\n",
      " \n",
      "Found  2  unique parameter combinations\n",
      " \n",
      "Clf  0 Final Accuracy: -0.2330 +/- 0.5800\n",
      "Clf  1 Final Accuracy: -0.0031 +/- 0.7661\n",
      " \n",
      "Using CLF with accuracy:   0.000000\n",
      "CLF params:  {'normalize': False, 'warm_start': False, 'selection': 'cyclic', 'fit_intercept': False, 'l1_ratio': 0.1, 'max_iter': 1000, 'precompute': False, 'random_state': None, 'tol': 0.0001, 'positive': False, 'copy_X': True, 'alpha': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElasticNet(alpha=1.0, copy_X=True, fit_intercept=False, l1_ratio=0.1,\n",
       "      max_iter=1000, normalize=False, positive=False, precompute=False,\n",
       "      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_grid = {'fit_intercept':[True,False],\n",
    "           'l1_ratio':[0.1,0.3,0.5,0.7,0.9]}\n",
    "rv.optimize_timeseries_reg( ElasticNet(), variable_values, target_values, my_grid )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold  1 accuracy: -0.2858  ,  -1.3955  {'alpha': 0.0001, 'hidden_layer_sizes': 100}\n",
      "Fold  2 accuracy: -2.0755  ,  -1.6481  {'alpha': 3.1622776601683795e-05, 'hidden_layer_sizes': 150}\n",
      "Fold  3 accuracy: -0.3176  ,  -0.4443  {'alpha': 3.1622776601683795e-05, 'hidden_layer_sizes': 200}\n",
      "Fold  4 accuracy: -1.9789  ,  -0.0365  {'alpha': 0.0001, 'hidden_layer_sizes': 200}\n",
      "Fold  5 accuracy: 0.2285  ,  -0.2336  {'alpha': 1.0000000000000001e-05, 'hidden_layer_sizes': 200}\n",
      "Fold  6 accuracy: -0.8961  ,  -0.0650  {'alpha': 3.1622776601683795e-05, 'hidden_layer_sizes': 200}\n",
      "Fold  7 accuracy: -0.4071  ,  0.0873  {'alpha': 0.0001, 'hidden_layer_sizes': 150}\n",
      "Fold  8 accuracy: -1.0916  ,  0.0813  {'alpha': 0.0001, 'hidden_layer_sizes': 200}\n",
      "Fold  9 accuracy: -0.1278  ,  0.1539  {'alpha': 0.00031622776601683794, 'hidden_layer_sizes': 200}\n",
      "Fold 10 accuracy: -0.1185  ,  0.2360  {'alpha': 0.00031622776601683794, 'hidden_layer_sizes': 200}\n",
      " \n",
      "Found  8  unique parameter combinations\n",
      " \n",
      "Clf  0 Final Accuracy: -0.4564 +/- 0.8144\n",
      "Clf  1 Final Accuracy: -0.1530 +/- 0.7474\n",
      "Clf  2 Final Accuracy: -0.1411 +/- 0.7349\n",
      "Clf  3 Final Accuracy: -0.1967 +/- 0.8232\n",
      "Clf  4 Final Accuracy: -0.1452 +/- 0.6913\n",
      "Clf  5 Final Accuracy: -0.1557 +/- 0.7334\n",
      "Clf  6 Final Accuracy: -0.0267 +/- 0.7450\n",
      "Clf  7 Final Accuracy: -0.0622 +/- 0.5498\n",
      " \n",
      "Using CLF with accuracy:   0.000000\n",
      "CLF params:  {'beta_1': 0.9, 'warm_start': False, 'beta_2': 0.999, 'shuffle': True, 'verbose': False, 'nesterovs_momentum': True, 'hidden_layer_sizes': 100, 'epsilon': 1e-08, 'activation': 'relu', 'max_iter': 200, 'batch_size': 'auto', 'power_t': 0.5, 'random_state': None, 'learning_rate_init': 0.001, 'tol': 0.0001, 'validation_fraction': 0.1, 'alpha': 0.0001, 'solver': 'adam', 'momentum': 0.9, 'learning_rate': 'constant', 'early_stopping': False}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=100, learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_grid = {'hidden_layer_sizes':[100,150,200],\n",
    "           'alpha':10**np.arange(-5,-3,0.5)}\n",
    "rv.optimize_timeseries_reg( MLPRegressor(), variable_values, target_values, my_grid )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
